{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, BertForTokenClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset, DatasetDict, Features, ClassLabel, Sequence, Value\n",
    "import re\n",
    "\n",
    "# Load the JSON file\n",
    "with open(\"../data/dialogues_fixed.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to extract all unique entity types\n",
    "def get_entity_types_from_data(data):\n",
    "    entity_types = set()  # Set to store unique entity types\n",
    "    \n",
    "    # Iterate over all dialogues and turns\n",
    "    for dialogue in data:\n",
    "        for turn in dialogue['turns']:\n",
    "            if 'entities' in turn:\n",
    "                for entity in turn['entities']:\n",
    "                    entity_types.add(entity['type'])\n",
    "    \n",
    "    return entity_types\n",
    "\n",
    "# Function to generate BIO labels for a sentence\n",
    "def create_bio_labels(sentence, entities, entity_types):\n",
    "    words = sentence.split()  # Tokenize the sentence into words\n",
    "    labels = ['O'] * len(words)  # Initialize all labels to 'O'\n",
    "    \n",
    "    for entity in entities:\n",
    "        start, end, entity_type = entity['start'], entity['end'], entity['type']\n",
    "        \n",
    "        # Set BIO labels for the entity span\n",
    "        for idx in range(start, end + 1):\n",
    "            if idx == start:\n",
    "                labels[idx] = f'B-{entity_type}'  # Begin of entity\n",
    "            else:\n",
    "                labels[idx] = f'I-{entity_type}'  # Inside entity\n",
    "                \n",
    "    return words, labels\n",
    "\n",
    "# Preprocessing for Intent Classification\n",
    "def preprocess_intent_data(data):\n",
    "    texts = []\n",
    "    intents = []\n",
    "\n",
    "    for dialogue in data:\n",
    "        for turn in dialogue['turns']:\n",
    "            text = turn.get('text', '')\n",
    "            intent = turn.get('intent', '')\n",
    "\n",
    "            texts.append(text)\n",
    "            intents.append(intent if intent else \"UNKNOWN\")  # Replace empty intents with \"UNKNOWN\"\n",
    "\n",
    "    # Convert intents to a set of unique labels for encoding\n",
    "    intent_labels = list(set(intents))\n",
    "    intent_labels.sort()  # Sort to ensure consistent label ordering\n",
    "    intent_label_map = {label: idx for idx, label in enumerate(intent_labels)}\n",
    "\n",
    "    # Encode the intents as numeric labels\n",
    "    encoded_intents = [intent_label_map[intent] for intent in intents]\n",
    "\n",
    "    intent_dataset = Dataset.from_dict({\n",
    "        'text': texts,\n",
    "        'intent': encoded_intents\n",
    "    })\n",
    "\n",
    "    # Add 'labels' key to dataset for the Trainer\n",
    "    intent_dataset = intent_dataset.map(lambda e: {'labels': e['intent']}, batched=True)\n",
    "    return intent_dataset, intent_label_map\n",
    "\n",
    "\n",
    "entity_types = get_entity_types_from_data(data)  # Implement this based on your dataset structure\n",
    "label_map = {f'B-{entity}': idx for idx, entity in enumerate(entity_types)}\n",
    "label_map.update({f'I-{entity}': idx + len(entity_types) for idx, entity in enumerate(entity_types)})\n",
    "label_map['O'] = len(entity_types) * 2  # 'O' (outside) label\n",
    "\n",
    "\n",
    "# Preprocessing for NER\n",
    "def preprocess_ner_data(data, tokenizer):\n",
    "    from datasets import Dataset\n",
    "\n",
    "    tokens_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for dialogue in data:\n",
    "        for turn in dialogue['turns']:\n",
    "            if \"entities\" in turn and turn['entities']:\n",
    "                sentence = turn['text']\n",
    "\n",
    "                # Tokenize with offsets for word alignment\n",
    "                tokens = tokenizer(\n",
    "                    sentence,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    return_offsets_mapping=True,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                input_ids = tokens[\"input_ids\"][0].tolist()  # Extract token IDs\n",
    "                offsets = tokens[\"offset_mapping\"][0].tolist()  # Character offsets\n",
    "                labels = [label_map[\"O\"]] * len(input_ids)  # Initialize all labels to 'O'\n",
    "\n",
    "                for entity in turn['entities']:\n",
    "                    entity_start = entity['start']\n",
    "                    entity_end = entity['end']\n",
    "                    entity_type = entity['type']\n",
    "\n",
    "                    # Map entity span to tokens\n",
    "                    for idx, (start, end) in enumerate(offsets):\n",
    "                        if start is None or end is None:  # Skip special tokens\n",
    "                            continue\n",
    "                        if start >= entity_start and end <= entity_end:\n",
    "                            if start == entity_start:  # Beginning of entity\n",
    "                                labels[idx] = label_map[f\"B-{entity_type}\"]\n",
    "                            else:  # Inside entity\n",
    "                                labels[idx] = label_map[f\"I-{entity_type}\"]\n",
    "\n",
    "                tokens_list.append(input_ids)\n",
    "                labels_list.append(labels)\n",
    "\n",
    "    # Return as a Hugging Face Dataset\n",
    "    return Dataset.from_dict({\n",
    "        'tokens': tokens_list,\n",
    "        'labels': labels_list\n",
    "    })\n",
    "\n",
    "\n",
    "# Prepare the datasets\n",
    "intent_dataset, intent_label_map = preprocess_intent_data(data)\n",
    "ner_dataset = preprocess_ner_data(data, tokenizer)\n",
    "\n",
    "# Check the size of ner_dataset\n",
    "print(f\"ner_dataset size: {len(ner_dataset)}\")\n",
    "\n",
    "# Ensure train_size is within bounds\n",
    "train_size = int(0.8 * len(ner_dataset))  # 80% for training\n",
    "\n",
    "# Split the ner_dataset for training and validation\n",
    "train_ner_dataset = ner_dataset.select(range(train_size))  # Selecting the first 80% of the data\n",
    "val_ner_dataset = ner_dataset.select(range(train_size, len(ner_dataset)))  # Selecting the remaining 20%\n",
    "\n",
    "# For intent dataset, split similarly\n",
    "train_size_intent = int(0.8 * len(intent_dataset))  # 80% for training\n",
    "train_intent_dataset = intent_dataset.select(range(train_size_intent))  # Selecting the first 80% of the intent data\n",
    "val_intent_dataset = intent_dataset.select(range(train_size_intent, len(intent_dataset)))  # Selecting the remaining 20%\n",
    "\n",
    "# Tokenize the datasets\n",
    "def tokenize_intent(examples):\n",
    "    return tokenizer(examples['text'], padding=True, truncation=True, max_length=128)\n",
    "\n",
    "def tokenize_ner(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to words\n",
    "        label_ids = []\n",
    "        previous_word_id = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:  # Special tokens\n",
    "                label_ids.append(-100)\n",
    "            elif word_id != previous_word_id:  # Beginning of a new word\n",
    "                label_ids.append(label[word_id])\n",
    "            else:  # Inside a word\n",
    "                label_ids.append(label[word_id])\n",
    "            previous_word_id = word_id\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "\n",
    "# Apply the tokenization\n",
    "train_intent_dataset = train_intent_dataset.map(tokenize_intent, batched=True)\n",
    "val_intent_dataset = val_intent_dataset.map(tokenize_intent, batched=True)\n",
    "\n",
    "train_ner_dataset = train_ner_dataset.map(tokenize_ner, batched=True)\n",
    "val_ner_dataset = val_ner_dataset.map(tokenize_ner, batched=True)\n",
    "\n",
    "# Define the model for Intent Classification\n",
    "intent_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(intent_label_map))\n",
    "\n",
    "# Define the model for NER\n",
    "ner_model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(label_map))  # Number of unique labels for NER\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate after each epoch\n",
    "    save_strategy=\"epoch\",  # Save after each epoch to match evaluation strategy\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',  # Directory for storing logs\n",
    "    logging_steps=500,  # Log every 500 steps\n",
    "    load_best_model_at_end=True,  # Load the best model based on evaluation metric\n",
    ")\n",
    "\n",
    "# Define the Trainer for Intent Classification\n",
    "intent_trainer = Trainer(\n",
    "    model=intent_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_intent_dataset,\n",
    "    eval_dataset=val_intent_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Define the Trainer for NER\n",
    "ner_trainer = Trainer(\n",
    "    model=ner_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ner_dataset,\n",
    "    eval_dataset=val_ner_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Training the models\n",
    "intent_trainer.train()\n",
    "ner_trainer.train()\n",
    "\n",
    "# Save the trained models\n",
    "intent_model.save_pretrained('./intent_model')\n",
    "ner_model.save_pretrained('./ner_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, '34567': 1, '12121': 2, '12 Nov 2024': 3, '121212': 4, '21 November 2024': 5, '5643': 6, '12345': 7, '67890': 8, '10 April 2024': 9, '23456': 10, '98765': 11, '6': 12, '987654': 13, '121243': 14, '987654321': 15, '5': 16, '12912': 17, '212142': 18, '54321': 19, '24 Oct 2024': 20, '212131413': 21, '121435': 22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent predictions: [9 9]\n",
      "NER predictions: [[1 1 1 ... 1 0 0]\n",
      " [1 1 1 ... 0 0 0]]\n",
      "Intent predictions: [9 6]\n",
      "NER predictions: [[1 1 1 ... 1 1 1]\n",
      " [1 1 0 ... 1 0 0]]\n",
      "Intent predictions: [6 6]\n",
      "NER predictions: [[1 1 1 ... 1 1 0]\n",
      " [1 1 0 ... 1 1 0]]\n",
      "Intent predictions: [6 9]\n",
      "NER predictions: [[0 1 0 ... 0 0 0]\n",
      " [1 1 0 ... 1 1 1]]\n",
      "Intent predictions: [6 6]\n",
      "NER predictions: [[0 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 1 0 0]]\n",
      "Intent predictions: [6 9]\n",
      "NER predictions: [[1 0 0 ... 1 0 1]\n",
      " [0 1 0 ... 0 1 0]]\n",
      "Intent predictions: [6 6]\n",
      "NER predictions: [[1 1 1 ... 1 0 1]\n",
      " [1 1 1 ... 1 1 1]]\n",
      "Intent predictions: [9 6]\n",
      "NER predictions: [[0 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 1 1 1]]\n",
      "Intent predictions: [6 9]\n",
      "NER predictions: [[1 0 1 ... 0 1 0]\n",
      " [1 1 1 ... 1 1 0]]\n",
      "Intent predictions: [6 9]\n",
      "NER predictions: [[0 1 1 ... 1 1 0]\n",
      " [1 1 1 ... 1 0 0]]\n",
      "Intent predictions: [6 6]\n",
      "NER predictions: [[1 1 1 ... 1 1 1]\n",
      " [0 0 1 ... 0 1 0]]\n",
      "Intent predictions: [9 6]\n",
      "NER predictions: [[1 1 0 ... 1 0 0]\n",
      " [1 1 1 ... 1 0 0]]\n",
      "Intent predictions: [9 6]\n",
      "NER predictions: [[1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]]\n",
      "Intent predictions: [6 6]\n",
      "NER predictions: [[1 1 1 ... 1 0 0]\n",
      " [1 0 1 ... 1 1 1]]\n",
      "Intent predictions: [6 9]\n",
      "NER predictions: [[0 1 0 ... 0 1 0]\n",
      " [1 1 1 ... 0 0 0]]\n",
      "Intent predictions: [6 6]\n",
      "NER predictions: [[1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 0]]\n",
      "Intent predictions: [9]\n",
      "NER predictions: [[0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1\n",
      "  0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0\n",
      "  1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0\n",
      "  0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0\n",
      "  0 0 1 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1\n",
      "  1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 1 0\n",
      "  0 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 1 1 1 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 1 1 1 0 0 0\n",
      "  0 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0\n",
      "  1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 0 1 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertForTokenClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load dialogue data from the JSON file\n",
    "dialogue_file_path = '../data/dialogues_fixed.json'\n",
    "\n",
    "with open(dialogue_file_path, 'r') as file:\n",
    "    dialogues = json.load(file)\n",
    "\n",
    "unique_entity_labels = set()\n",
    "for dialogue in dialogues:\n",
    "    for turn in dialogue[\"turns\"]:\n",
    "        if \"entities\" in turn:\n",
    "            for entity in turn[\"entities\"]:\n",
    "                unique_entity_labels.add(entity[\"entity\"])\n",
    "\n",
    "unique_intent_labels = set()\n",
    "for dialogue in dialogues:\n",
    "    for turn in dialogue[\"turns\"]:\n",
    "        if \"intent\" in turn:\n",
    "            unique_intent_labels.add(turn[\"intent\"])\n",
    "\n",
    "intent_label_map = {label: idx for idx, label in enumerate(unique_intent_labels)}\n",
    "\n",
    "# Now create the label map, assigning unique integers to each label\n",
    "entity_label_map = {\"O\": 0}  # Add \"O\" for padding or non-entities first\n",
    "for idx, label in enumerate(unique_entity_labels, start=1):\n",
    "    entity_label_map[label] = idx\n",
    "\n",
    "print(entity_label_map)\n",
    "\n",
    "# Dataset for intent classification and NER\n",
    "class DialogueDataset(Dataset):\n",
    "    def __init__(self, dialogues, tokenizer, max_length=512, max_entities_length=6):\n",
    "        self.dialogues = dialogues\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.max_entities_length = max_entities_length  # Set max_entities_length for padding\n",
    "\n",
    "        # Define entity label mapping (e.g., \"O\" -> 0, \"entity1\" -> 1, etc.)\n",
    "        self.entity_label_map = entity_label_map\n",
    "        self.intent_label_map = intent_label_map\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dialogues)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        dialogue = self.dialogues[idx]\n",
    "        turns = dialogue[\"turns\"]\n",
    "        \n",
    "        # Concatenate previous turns with the current turn for intent classification\n",
    "        dialogue_history = \" \".join([turn[\"text\"] for turn in turns[:-1]])  # All except the last turn\n",
    "        current_turn = turns[-1][\"text\"]  # Last turn (usually the user's query)\n",
    "        \n",
    "        # Combine dialogue history and current turn (concatenating previous dialogue turns)\n",
    "        input_text = dialogue_history + \" \" + current_turn\n",
    "        \n",
    "        # Tokenize the input text\n",
    "        encoding = self.tokenizer(input_text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        \n",
    "        # Intent label (extract the intent of the current turn)\n",
    "        intent_label = turns[-1].get(\"intent\", None)  # Could be None if the Bot turn is considered\n",
    "        if intent_label is None:\n",
    "            intent_label = \"default\"  # Default label if no intent is present\n",
    "        \n",
    "        # Use an intent label map to convert string intent labels to integers\n",
    "        intent_label = self.intent_label_map.get(intent_label, 0)  # 0 as default if not found\n",
    "        \n",
    "        # Named Entity Recognition: Extract entities from current user turn\n",
    "        entities = turns[-1].get(\"entities\", [])\n",
    "        entity_labels = [entity[\"entity\"] for entity in entities]  # List of entities (e.g., order number)\n",
    "        \n",
    "        # Pad entity labels to self.max_entities_length (use \"O\" for padding)\n",
    "        padded_entity_labels = entity_labels + [\"O\"] * (self.max_entities_length - len(entity_labels))\n",
    "        \n",
    "        # If entities are fewer than max_entities_length, pad the rest with \"O\"\n",
    "        # If entities are more than max_entities_length, truncate the list to ensure that it fits within the specified length\n",
    "        padded_entity_labels = padded_entity_labels[:self.max_entities_length]\n",
    "        \n",
    "        # Convert entity labels to integers using the entity label map\n",
    "        integer_entity_labels = [self.entity_label_map.get(label, 0) for label in padded_entity_labels]  # Default to \"O\" (0)\n",
    "        \n",
    "        # Convert entity labels to tensors\n",
    "        padded_entity_labels_tensor = torch.tensor(integer_entity_labels)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding['input_ids'].squeeze(),\n",
    "            \"attention_mask\": encoding['attention_mask'].squeeze(),\n",
    "            \"intent_label\": intent_label,  # Now intent_label is an integer\n",
    "            \"entities\": padded_entity_labels_tensor\n",
    "        }\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "dataset = DialogueDataset(dialogues, tokenizer)\n",
    "\n",
    "# Define a function to handle the batching process and avoid NoneType errors\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    intent_labels = [item['intent_label'] for item in batch]\n",
    "    entity_labels = [item['entities'] for item in batch]\n",
    "    \n",
    "    # Handle padding for entity labels\n",
    "    max_entities_length = max(len(entities) for entities in entity_labels)\n",
    "    \n",
    "    # Convert entity labels to a list if they are tensors\n",
    "    if isinstance(entity_labels[0], torch.Tensor):\n",
    "        entity_labels = [entities.tolist() for entities in entity_labels]\n",
    "    \n",
    "    padded_entity_labels = [entities + [\"O\"] * (max_entities_length - len(entities)) for entities in entity_labels]\n",
    "    \n",
    "    # Convert padded_entity_labels back to a tensor\n",
    "    padded_entity_labels_tensor = torch.tensor(padded_entity_labels)\n",
    "    \n",
    "    # Convert intent labels to tensor (ensure they are integers)\n",
    "    intent_labels_tensor = torch.tensor(intent_labels, dtype=torch.long)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"intent_labels\": intent_labels_tensor,\n",
    "        \"entity_labels\": padded_entity_labels_tensor\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Define DataLoader for batching\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Load the pre-trained models (intent classification and NER)\n",
    "intent_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(unique_intent_labels))  # Adjust num_labels based on your intents\n",
    "ner_model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=)  # Adjust num_labels based on NER classes (e.g., 'O', 'order_number')\n",
    "\n",
    "# Define a function to predict intent and NER from the model\n",
    "def predict_intent_and_entities(input_ids, attention_mask):\n",
    "    # Predict intent using the intent model\n",
    "    intent_outputs = intent_model(input_ids, attention_mask=attention_mask)\n",
    "    intent_preds = torch.argmax(intent_outputs.logits, dim=1)\n",
    "    \n",
    "    # Predict NER using the NER model\n",
    "    ner_outputs = ner_model(input_ids, attention_mask=attention_mask)\n",
    "    ner_preds = torch.argmax(ner_outputs.logits, dim=2)\n",
    "    \n",
    "    return intent_preds, ner_preds\n",
    "\n",
    "# Process the DataLoader\n",
    "for batch in dataloader:\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    \n",
    "    # Get intent and NER predictions\n",
    "    intent_preds, ner_preds = predict_intent_and_entities(input_ids, attention_mask)\n",
    "    \n",
    "    # Convert predictions to readable format\n",
    "    intent_labels = intent_preds.numpy()\n",
    "    ner_labels = ner_preds.numpy()\n",
    "    \n",
    "    print(f\"Intent predictions: {intent_labels}\")\n",
    "    print(f\"NER predictions: {ner_labels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1031,  5310,  1033,  2064,  2017,  2393,  2033,  2650,  2026,\n",
      "          2344,  1029,   102,  2469,   999,  2071,  2017,  3073,  2033,  2007,\n",
      "          2115,  2344,  2193,  1029,   102,  2009,  1005,  1055, 13138, 19961,\n",
      "          1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example conversation history\n",
    "text = \"[USER] Can you help me track my order? [SEP] Sure! Could you provide me with your order number? [SEP] It's 12345.\"\n",
    "\n",
    "# Tokenize the text\n",
    "inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# The tokenized output will include input_ids, attention_mask, etc.\n",
    "print(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted intent: 2\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=10)\n",
    "\n",
    "# Define the input and tokenize it\n",
    "inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Get predictions\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "\n",
    "# Get predicted intent\n",
    "predicted_class_id = torch.argmax(logits, dim=1).item()\n",
    "print(\"Predicted intent:\", predicted_class_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted entities: [{'entity': '[CLS]', 'type': 'B-ORDER_NUMBER'}, {'entity': '[', 'type': 'B-ORDER_NUMBER'}, {'entity': '[SEP]', 'type': 'B-ORDER_NUMBER'}, {'entity': 'sure', 'type': 'B-ORDER_NUMBER'}, {'entity': '!', 'type': 'B-ORDER_NUMBER'}, {'entity': 'number', 'type': 'B-ORDER_NUMBER'}, {'entity': '[SEP]', 'type': 'B-ORDER_NUMBER'}, {'entity': \"'\", 'type': 'B-ORDER_NUMBER'}, {'entity': 's', 'type': 'B-ORDER_NUMBER'}, {'entity': '123', 'type': 'B-ORDER_NUMBER'}, {'entity': '.', 'type': 'B-ORDER_NUMBER'}, {'entity': '[SEP]', 'type': 'B-ORDER_NUMBER'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model for token classification\n",
    "ner_model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Define the input and tokenize it\n",
    "inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Get predictions for each token\n",
    "outputs = ner_model(**inputs)\n",
    "logits = outputs.logits\n",
    "\n",
    "# Convert logits to predicted labels\n",
    "predicted_labels = torch.argmax(logits, dim=2).squeeze().tolist()\n",
    "\n",
    "# Map labels to entities (you would need a mapping of label IDs to entity names)\n",
    "label_map = {0: \"O\", 1: \"B-ORDER_NUMBER\", 2: \"I-ORDER_NUMBER\"}\n",
    "entities = []\n",
    "\n",
    "# Extract entities\n",
    "for token_id, label_id in zip(inputs.input_ids[0], predicted_labels):\n",
    "    token = tokenizer.decode([token_id])\n",
    "    label = label_map[label_id]\n",
    "    if label != \"O\":\n",
    "        entities.append({\"entity\": token, \"type\": label})\n",
    "\n",
    "print(\"Extracted entities:\", entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for maintaining dialogue history\n",
    "history = []\n",
    "history.append(\"[USER] Can you help me track my order?\")\n",
    "history.append(\"[BOT] Sure! Could you provide me with your order number?\")\n",
    "history.append(\"[USER] It's 12345.\")\n",
    "history_text = \" [SEP] \".join(history)\n",
    "inputs = tokenizer(history_text, padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harshit/IITH/Sem7/CyberSecurityAndAI/cyber_security_project/chatbot_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text:\n",
      "[User] Can you help me track my order? [SEP] [Bot] Sure! Could you provide me with your order number? [SEP] [User] It's 12345. [SEP] [Bot] Thank you! Let me check the status of order 12345.\n",
      "\n",
      "Intent Labels:\n",
      "['track_order', 'track_order.give_order_id']\n",
      "\n",
      "Entity Labels:\n",
      "[['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-order_number', 'I-order_number', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Sample dialogue data\n",
    "dialogue = {\n",
    "    \"dialogue_id\": 8,\n",
    "    \"turns\": [\n",
    "        {\n",
    "            \"speaker\": \"User\",\n",
    "            \"text\": \"Can you help me track my order?\",\n",
    "            \"intent\": \"track_order\",\n",
    "            \"entities\": []\n",
    "        },\n",
    "        {\n",
    "            \"speaker\": \"Bot\",\n",
    "            \"text\": \"Sure! Could you provide me with your order number?\"\n",
    "        },\n",
    "        {\n",
    "            \"speaker\": \"User\",\n",
    "            \"text\": \"It's 12345.\",\n",
    "            \"intent\": \"give_order_id\",\n",
    "            \"entities\": [\n",
    "                {\n",
    "                    \"entity\": \"12345\",\n",
    "                    \"type\": \"order_number\",\n",
    "                    \"start\": 4,\n",
    "                    \"end\": 9\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"speaker\": \"Bot\",\n",
    "            \"text\": \"Thank you! Let me check the status of order 12345.\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Preprocess function\n",
    "def preprocess_dialogue(dialogue):\n",
    "    dialogue_history = []\n",
    "    intent_labels = []\n",
    "    entity_labels = []\n",
    "\n",
    "    # Iterate through turns to build dialogue history and extract labels\n",
    "    primary_label = None\n",
    "    for turn in dialogue['turns']:\n",
    "        text = turn['text']\n",
    "        speaker = turn['speaker']\n",
    "\n",
    "        # Combine the speaker's text in the form of [SPEAKER] message\n",
    "        dialogue_history.append(f\"[{speaker}] {text}\")\n",
    "\n",
    "        # If it's the user turn and there are entities, process them\n",
    "        if speaker == \"User\":\n",
    "            # Tokenize the text\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "            tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "            # Entity tagging (For example, \"12345\" as order_number)\n",
    "            token_labels = [\"O\"] * len(tokens)  # \"O\" means no entity\n",
    "            \n",
    "            for entity in turn.get(\"entities\", []):\n",
    "                entity_text = entity[\"entity\"]\n",
    "                entity_type = entity[\"type\"]\n",
    "                start = entity[\"start\"]\n",
    "                end = entity[\"end\"]\n",
    "\n",
    "                # Match the entity text with the tokenized form\n",
    "                entity_tokens = tokenizer.tokenize(entity_text)  # Subword tokens of the entity\n",
    "                entity_token_count = len(entity_tokens)\n",
    "\n",
    "                # Iterate through the tokens and mark the entity span\n",
    "                for idx, token in enumerate(tokens):\n",
    "                    if token == entity_tokens[0]:  # Found the start token of the entity\n",
    "                        token_labels[idx] = f\"B-{entity_type}\"  # Beginning of the entity\n",
    "                        for j in range(1, entity_token_count):\n",
    "                            token_labels[idx + j] = f\"I-{entity_type}\"  # Inside the entity\n",
    "\n",
    "            # Append the labels for entity extraction\n",
    "            entity_labels.append(token_labels)\n",
    "\n",
    "            # Append the intent label for user turns\n",
    "            if primary_label is not None:\n",
    "                intent_labels.append(f\"{primary_label}.{turn['intent']}\")\n",
    "            else:\n",
    "                primary_label = turn['intent']\n",
    "                intent_labels.append(turn['intent'])\n",
    "\n",
    "    # Join dialogue history into one input sequence\n",
    "    input_text = \" [SEP] \".join(dialogue_history)\n",
    "\n",
    "    return input_text, intent_labels, entity_labels\n",
    "\n",
    "# Get the preprocessed input, labels for intent and entities\n",
    "input_text, intent_labels, entity_labels = preprocess_dialogue(dialogue)\n",
    "\n",
    "# Display the preprocessed data\n",
    "print(\"Input Text:\")\n",
    "print(input_text)\n",
    "\n",
    "print(\"\\nIntent Labels:\")\n",
    "print(intent_labels)\n",
    "\n",
    "print(\"\\nEntity Labels:\")\n",
    "print(entity_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harshit/IITH/Sem7/CyberSecurityAndAI/cyber_security_project/chatbot_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_dialogue' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData preprocessing complete! Processed data saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/processed_dialogues.json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Run the preprocessing function\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[43mprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m, in \u001b[0;36mprocess_dataset\u001b[0;34m(data_path)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Process each dialogue\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dialogue \u001b[38;5;129;01min\u001b[39;00m dialogues_data:\n\u001b[0;32m---> 23\u001b[0m     input_text, intent_labels, entity_labels \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_dialogue\u001b[49m(dialogue)\n\u001b[1;32m     24\u001b[0m     inputs\u001b[38;5;241m.\u001b[39mappend(input_text)\n\u001b[1;32m     25\u001b[0m     intents\u001b[38;5;241m.\u001b[39mappend(intent_labels)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocess_dialogue' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the dialogues data from the provided path\n",
    "data_path = '../data/dialogues_fixed.json'\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to process the entire dataset and save it\n",
    "def process_dataset(data_path):\n",
    "    # Load the dataset from the JSON file\n",
    "    with open(data_path, 'r') as f:\n",
    "        dialogues_data = json.load(f)\n",
    "\n",
    "    # Initialize lists to hold the processed data\n",
    "    inputs = []\n",
    "    intents = []\n",
    "    entities = []\n",
    "\n",
    "    # Process each dialogue\n",
    "    for dialogue in dialogues_data:\n",
    "        input_text, intent_labels, entity_labels = preprocess_dialogue(dialogue)\n",
    "        inputs.append(input_text)\n",
    "        intents.append(intent_labels)\n",
    "        entities.append(entity_labels)\n",
    "\n",
    "    # Save the processed data to a new JSON file or CSV\n",
    "    processed_data = {\n",
    "        \"inputs\": inputs,\n",
    "        \"intents\": intents,\n",
    "        \"entities\": entities\n",
    "    }\n",
    "\n",
    "    # Save as JSON\n",
    "    with open('../data/processed_dialogues.json', 'w') as f:\n",
    "        json.dump(processed_data, f, indent=4)\n",
    "\n",
    "    print(f\"Data preprocessing complete! Processed data saved to '../data/processed_dialogues.json'\")\n",
    "\n",
    "# Run the preprocessing function\n",
    "process_dataset(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Map: {'B-count': 0, 'B-end_date': 2, 'B-order_id': 4, 'B-order_number': 6, 'B-start_date': 8, 'I-count': 1, 'I-end_date': 3, 'I-order_id': 5, 'I-order_number': 7, 'I-start_date': 9, 'O': 10}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the raw dataset (dialogues_fixed.json)\n",
    "with open('../data/dialogues_fixed.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract all entity types from the dataset\n",
    "entity_types = set()\n",
    "\n",
    "# Iterate through the dialogues and extract entity types\n",
    "for dialogue in data:\n",
    "    for turn in dialogue['turns']:\n",
    "        if 'entities' in turn:\n",
    "            for entity in turn['entities']:\n",
    "                # Add the entity type to the set\n",
    "                entity_types.add(entity['type'])\n",
    "\n",
    "# Create a mapping from entity type to a unique ID\n",
    "entity_map = {f\"B-{entity}\": idx*2 for idx, entity in enumerate(sorted(entity_types))}\n",
    "entity_map.update({f\"I-{entity}\": idx*2+1 for idx, entity in enumerate(sorted(entity_types))})\n",
    "\n",
    "# 'O' (Outside) will have a special index for non-entity tokens\n",
    "entity_map['O'] = len(entity_map)\n",
    "\n",
    "# Print the resulting entity map\n",
    "print(\"Entity Map:\", entity_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from transformers import BertTokenizer, BertForSequenceClassification, BertForTokenClassification, AdamW\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from tqdm import tqdm\n",
    "# import json\n",
    "\n",
    "# # Load the tokenizer and model for BERT\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Load the preprocessed dataset\n",
    "# with open('../data/processed_dialogues.json', 'r') as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# with open('../data/dialogues_fixed.json', 'r') as f:\n",
    "#     complete_data = json.load(f)\n",
    "\n",
    "# # Define the dataset class for loading the data\n",
    "# class MultiTaskDataset(Dataset):\n",
    "#     def __init__(self, inputs, intents, entities, tokenizer, intent_map, entity_map, max_length=512):\n",
    "#         self.inputs = inputs\n",
    "#         self.intents = intents\n",
    "#         self.entities = entities\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.intent_map = intent_map\n",
    "#         self.entity_map = entity_map\n",
    "#         self.max_length = max_length\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.inputs)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         input_text = self.inputs[idx]\n",
    "#         intent_label = self.intents[idx]\n",
    "#         entity_labels = self.entities[idx]\n",
    "\n",
    "#         # Tokenize the input text for BERT\n",
    "#         encoding = self.tokenizer(input_text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\")\n",
    "\n",
    "#         # Prepare token labels for entity extraction, mapping to integer IDs\n",
    "#         entity_labels_padded = [self.entity_map.get(entity, self.entity_map['O']) for entity in entity_labels]  # Default 'O' (0) if not found\n",
    "#         entity_labels_padded = entity_labels_padded + [self.entity_map['O']] * (self.max_length - len(entity_labels_padded))  # Padding\n",
    "\n",
    "#         # Convert intent labels to numeric IDs\n",
    "#         intent_label = self.intent_map.get(intent_label[-1], -1)  # Ensure intent is mapped\n",
    "\n",
    "#         # Convert everything to tensors\n",
    "#         input_ids = encoding['input_ids'].squeeze(0)\n",
    "#         attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "\n",
    "#         return {\n",
    "#             'input_ids': input_ids,\n",
    "#             'attention_mask': attention_mask,\n",
    "#             'intent_labels': torch.tensor(intent_label, dtype=torch.long),\n",
    "#             'entity_labels': torch.tensor(entity_labels_padded, dtype=torch.long)\n",
    "#         }\n",
    "\n",
    "# # Prepare the dataset\n",
    "# inputs = data['inputs']\n",
    "# intents = data['intents']\n",
    "# entities = data['entities']\n",
    "\n",
    "# # For simplicity, let's assume we have a simple mapping of intent labels and entity labels\n",
    "# intent_map = {}\n",
    "# # get all unique intents from complete_data\n",
    "# unique_intents = set()\n",
    "# for dialogue in complete_data:\n",
    "#     current_intent = None\n",
    "#     for idx, turn in enumerate(dialogue['turns']):\n",
    "#         if 'intent' in turn:\n",
    "#             if current_intent is not None:\n",
    "#                 unique_intents.add(f\"{current_intent}.{turn['intent']}\")\n",
    "#             else:\n",
    "#                 current_intent = turn['intent']\n",
    "#                 unique_intents.add(current_intent)\n",
    "# unique_intents = list(unique_intents)\n",
    "# intent_map = {intent: idx for idx, intent in enumerate(unique_intents)}\n",
    "\n",
    "# # Convert intents and entities to numeric labels\n",
    "# numeric_intents = [[intent_map[intent] for intent in sublist] for sublist in intents]\n",
    "# # Convert entities to numeric labels\n",
    "# numeric_entities = [\n",
    "#     [entity_map.get(entity, 10) for entity in sublist]  # Default to 'O' (10) if entity not found\n",
    "#     for sublist in [sub for sentence in entities for sub in sentence]  # Flatten the list of lists\n",
    "# ]\n",
    "\n",
    "\n",
    "# # Create the DataLoader for training\n",
    "# dataset = MultiTaskDataset(inputs, numeric_intents, numeric_entities, tokenizer, intent_map, entity_map)\n",
    "# dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# # Define the BERT models for both tasks\n",
    "# class MultiTaskBERTModel(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(MultiTaskBERTModel, self).__init__()\n",
    "#         self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(intent_map))\n",
    "#         self.ner_head = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(entity_map))\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         # Intent classification\n",
    "#         intent_logits = self.bert(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "#         # Entity recognition\n",
    "#         ner_logits = self.ner_head(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "#         return intent_logits, ner_logits\n",
    "\n",
    "# # Initialize the model\n",
    "# model = MultiTaskBERTModel()\n",
    "\n",
    "# # Define the optimizer\n",
    "# optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# # Define training loop\n",
    "# def train(model, dataloader, optimizer, device):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     accumulation_steps = 4  # Gradients will be accumulated over 4 steps\n",
    "#     optimizer.zero_grad()  # Zero gradients at the start\n",
    "\n",
    "#     for step, batch in enumerate(tqdm(dataloader)):\n",
    "#         # Move batch to device (GPU/CPU)\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "#         intent_labels = batch['intent_labels'].to(device)\n",
    "#         entity_labels = batch['entity_labels'].to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         intent_logits, ner_logits = model(input_ids, attention_mask)\n",
    "\n",
    "#         # Calculate losses\n",
    "#         intent_loss = torch.nn.CrossEntropyLoss()(intent_logits.view(-1, len(intent_map)), intent_labels.view(-1))\n",
    "#         ner_loss = torch.nn.CrossEntropyLoss()(ner_logits.view(-1, len(entity_map)), entity_labels.view(-1))\n",
    "        \n",
    "#         # Combine losses\n",
    "#         loss = intent_loss + ner_loss\n",
    "#         loss = loss / accumulation_steps  # Normalize the loss for gradient accumulation\n",
    "\n",
    "#         loss.backward()  # Backpropagate the loss\n",
    "\n",
    "#         # Accumulate gradients and step only after `accumulation_steps`\n",
    "#         if (step + 1) % accumulation_steps == 0:\n",
    "#             optimizer.step()\n",
    "#             optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "#         # Clean up memory after each step\n",
    "#         del input_ids, attention_mask, intent_labels, entity_labels\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "# # Training loop\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model.to(device)\n",
    "\n",
    "# epochs = 3\n",
    "# for epoch in range(epochs):\n",
    "#     avg_loss = train(model, dataloader, optimizer, device)\n",
    "#     print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# # Save the trained model\n",
    "# model.save_pretrained('../models/multitask_bert')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1732719697.768598    6903 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6250 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can't convert non-rectangular Python sequence to Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/IITH/Sem7/CyberSecurityAndAI/cyber_security_project/chatbot_venv/lib/python3.12/site-packages/tensorflow/python/data/util/structure.py:105\u001b[0m, in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    104\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     spec \u001b[38;5;241m=\u001b[39m \u001b[43mtype_spec_from_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m   \u001b[38;5;66;03m# TypeError indicates it was not possible to compute a `TypeSpec` for\u001b[39;00m\n\u001b[1;32m    108\u001b[0m   \u001b[38;5;66;03m# the value. As a fallback try converting the value to a tensor.\u001b[39;00m\n",
      "File \u001b[0;32m~/IITH/Sem7/CyberSecurityAndAI/cyber_security_project/chatbot_venv/lib/python3.12/site-packages/tensorflow/python/data/util/structure.py:514\u001b[0m, in \u001b[0;36mtype_spec_from_value\u001b[0;34m(element, use_fallback)\u001b[0m\n\u001b[1;32m    511\u001b[0m     logging\u001b[38;5;241m.\u001b[39mvlog(\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to convert \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m to tensor: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mtype\u001b[39m(element)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, e))\n\u001b[0;32m--> 514\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not build a `TypeSpec` for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m with type \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    515\u001b[0m     element,\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28mtype\u001b[39m(element)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not build a `TypeSpec` for [[2, 3], [2], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [2], [0], [0], [0], [0], [], [0], [0]] with type list",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 73\u001b[0m\n\u001b[1;32m     67\u001b[0m numeric_entities \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     68\u001b[0m     [entity_map\u001b[38;5;241m.\u001b[39mget(entity, \u001b[38;5;241m10\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m entity \u001b[38;5;129;01min\u001b[39;00m sublist]  \u001b[38;5;66;03m# Default 'O' (10) if not found\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m [sub \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m entities \u001b[38;5;28;01mfor\u001b[39;00m sub \u001b[38;5;129;01min\u001b[39;00m sentence]  \u001b[38;5;66;03m# Flatten the list of lists\u001b[39;00m\n\u001b[1;32m     70\u001b[0m ]\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Create the TensorFlow Dataset\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mMultiTaskDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_intents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_entities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintent_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentity_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Define the TensorFlow model for multi-task learning\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 38\u001b[0m, in \u001b[0;36mMultiTaskDataset.__new__\u001b[0;34m(cls, inputs, intents, entities, tokenizer, intent_map, entity_map, max_length)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     33\u001b[0m         {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: encoding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: encoding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]},\n\u001b[1;32m     34\u001b[0m         {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintent_labels\u001b[39m\u001b[38;5;124m'\u001b[39m: intent_label, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity_labels\u001b[39m\u001b[38;5;124m'\u001b[39m: entity_labels_padded}\n\u001b[1;32m     35\u001b[0m     )\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Create TensorFlow Dataset from the data\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentities\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x, y, z: _parse_function(x, y, z))\n\u001b[1;32m     40\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m8\u001b[39m)\n",
      "File \u001b[0;32m~/IITH/Sem7/CyberSecurityAndAI/cyber_security_project/chatbot_venv/lib/python3.12/site-packages/tensorflow/python/data/ops/dataset_ops.py:827\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;66;03m# from_tensor_slices_op -> dataset_ops).\u001b[39;00m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_tensor_slices_op\n\u001b[0;32m--> 827\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrom_tensor_slices_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/IITH/Sem7/CyberSecurityAndAI/cyber_security_project/chatbot_venv/lib/python3.12/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py:25\u001b[0m, in \u001b[0;36m_from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_tensor_slices\u001b[39m(tensors, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 25\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_TensorSliceDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/IITH/Sem7/CyberSecurityAndAI/cyber_security_project/chatbot_venv/lib/python3.12/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py:33\u001b[0m, in \u001b[0;36m_TensorSliceDataset.__init__\u001b[0;34m(self, element, is_files, name)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, element, is_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"See `Dataset.from_tensor_slices` for details.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m   element \u001b[38;5;241m=\u001b[39m \u001b[43mstructure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m   batched_spec \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mtype_spec_from_value(element)\n\u001b[1;32m     35\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_batched_tensor_list(batched_spec, element)\n",
      "File \u001b[0;32m~/IITH/Sem7/CyberSecurityAndAI/cyber_security_project/chatbot_venv/lib/python3.12/site-packages/tensorflow/python/data/util/structure.py:110\u001b[0m, in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    105\u001b[0m     spec \u001b[38;5;241m=\u001b[39m type_spec_from_value(t, use_fallback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m   \u001b[38;5;66;03m# TypeError indicates it was not possible to compute a `TypeSpec` for\u001b[39;00m\n\u001b[1;32m    108\u001b[0m   \u001b[38;5;66;03m# the value. As a fallback try converting the value to a tensor.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m   normalized_components\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 110\u001b[0m       \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomponent_\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m   \u001b[38;5;66;03m# To avoid a circular dependency between dataset_ops and structure,\u001b[39;00m\n\u001b[1;32m    113\u001b[0m   \u001b[38;5;66;03m# we check the class name instead of using `isinstance`.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetSpec\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/IITH/Sem7/CyberSecurityAndAI/cyber_security_project/chatbot_venv/lib/python3.12/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/IITH/Sem7/CyberSecurityAndAI/cyber_security_project/chatbot_venv/lib/python3.12/site-packages/tensorflow/python/framework/ops.py:732\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;66;03m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[1;32m    731\u001b[0m preferred_dtype \u001b[38;5;241m=\u001b[39m preferred_dtype \u001b[38;5;129;01mor\u001b[39;00m dtype_hint\n\u001b[0;32m--> 732\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccepted_result_types\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/IITH/Sem7/CyberSecurityAndAI/cyber_security_project/chatbot_venv/lib/python3.12/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    225\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    226\u001b[0m           _add_error_prefix(\n\u001b[1;32m    227\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    231\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m    237\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/IITH/Sem7/CyberSecurityAndAI/cyber_security_project/chatbot_venv/lib/python3.12/site-packages/tensorflow/python/framework/constant_tensor_conversion.py:29\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constant_op  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[1;32m     28\u001b[0m _ \u001b[38;5;241m=\u001b[39m as_ref\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/IITH/Sem7/CyberSecurityAndAI/cyber_security_project/chatbot_venv/lib/python3.12/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m~/IITH/Sem7/CyberSecurityAndAI/cyber_security_project/chatbot_venv/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py:276\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(\n\u001b[1;32m    179\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    180\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ops\u001b[38;5;241m.\u001b[39mOperation, ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase]:\n\u001b[1;32m    181\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/IITH/Sem7/CyberSecurityAndAI/cyber_security_project/chatbot_venv/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py:289\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    288\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 289\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39m_create_graph_constant(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    292\u001b[0m     value, dtype, shape, name, verify_shape, allow_broadcast\n\u001b[1;32m    293\u001b[0m )\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m const_tensor\n",
      "File \u001b[0;32m~/IITH/Sem7/CyberSecurityAndAI/cyber_security_project/chatbot_venv/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py:301\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(\n\u001b[1;32m    298\u001b[0m     ctx, value, dtype, shape, verify_shape\n\u001b[1;32m    299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase:\n\u001b[1;32m    300\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 301\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/IITH/Sem7/CyberSecurityAndAI/cyber_security_project/chatbot_venv/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't convert non-rectangular Python sequence to Tensor."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, TFBertForTokenClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "with open('../data/processed_dialogues.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open('../data/dialogues_fixed.json', 'r') as f:\n",
    "    complete_data = json.load(f)\n",
    "\n",
    "# Define the dataset class for TensorFlow\n",
    "class MultiTaskDataset(tf.data.Dataset):\n",
    "    def __new__(cls, inputs, intents, entities, tokenizer, intent_map, entity_map, max_length=512):\n",
    "        # Tokenize the input text\n",
    "        def _parse_function(input_text, intent_label, entity_labels):\n",
    "            encoding = tokenizer(input_text, truncation=True, padding='max_length', max_length=max_length, return_tensors=\"tf\")\n",
    "\n",
    "            # Convert entity labels to padded labels\n",
    "            entity_labels_padded = [entity_map.get(entity, entity_map['O']) for entity in entity_labels]\n",
    "            entity_labels_padded += [entity_map['O']] * (max_length - len(entity_labels_padded))\n",
    "\n",
    "            intent_label = intent_map.get(intent_label[-1], -1)\n",
    "\n",
    "            return (\n",
    "                {'input_ids': encoding['input_ids'], 'attention_mask': encoding['attention_mask']},\n",
    "                {'intent_labels': intent_label, 'entity_labels': entity_labels_padded}\n",
    "            )\n",
    "        \n",
    "        # Create TensorFlow Dataset from the data\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((inputs, intents, entities))\n",
    "        dataset = dataset.map(lambda x, y, z: _parse_function(x, y, z))\n",
    "        dataset = dataset.batch(8)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "# Prepare the dataset\n",
    "inputs = data['inputs']\n",
    "intents = data['intents']\n",
    "entities = data['entities']\n",
    "\n",
    "# For simplicity, let's assume we have a simple mapping of intent labels and entity labels\n",
    "intent_map = {}\n",
    "# get all unique intents from complete_data\n",
    "unique_intents = set()\n",
    "for dialogue in complete_data:\n",
    "    current_intent = None\n",
    "    for idx, turn in enumerate(dialogue['turns']):\n",
    "        if 'intent' in turn:\n",
    "            if current_intent is not None:\n",
    "                unique_intents.add(f\"{current_intent}.{turn['intent']}\")\n",
    "            else:\n",
    "                current_intent = turn['intent']\n",
    "                unique_intents.add(current_intent)\n",
    "unique_intents = list(unique_intents)\n",
    "intent_map = {intent: idx for idx, intent in enumerate(unique_intents)}\n",
    "\n",
    "# Convert intents and entities to numeric labels\n",
    "numeric_intents = [[intent_map[intent] for intent in sublist] for sublist in intents]\n",
    "numeric_entities = [\n",
    "    [entity_map.get(entity, 10) for entity in sublist]  # Default 'O' (10) if not found\n",
    "    for sublist in [sub for sentence in entities for sub in sentence]  # Flatten the list of lists\n",
    "]\n",
    "\n",
    "# Create the TensorFlow Dataset\n",
    "dataset = MultiTaskDataset(inputs, numeric_intents, numeric_entities, tokenizer, intent_map, entity_map)\n",
    "dataset = dataset.batch(4)\n",
    "\n",
    "# Define the TensorFlow model for multi-task learning\n",
    "class MultiTaskBERTModel(tf.keras.Model):\n",
    "    def __init__(self, intent_map, entity_map):\n",
    "        super(MultiTaskBERTModel, self).__init__()\n",
    "        # Load BERT models for sequence classification and token classification\n",
    "        self.bert_sequence = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(intent_map))\n",
    "        self.bert_token = TFBertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(entity_map))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Get the BERT model outputs for both tasks\n",
    "        outputs_seq = self.bert_sequence(**inputs)\n",
    "        outputs_token = self.bert_token(**inputs)\n",
    "\n",
    "        return outputs_seq.logits, outputs_token.logits\n",
    "\n",
    "# Initialize the model\n",
    "model = MultiTaskBERTModel(intent_map, entity_map)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = Adam(learning_rate=2e-5)\n",
    "\n",
    "# Loss function for intent and entity tasks\n",
    "loss_fn_intent = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "loss_fn_entity = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Training step function\n",
    "@tf.function\n",
    "def train_step(inputs, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        intent_logits, ner_logits = model(inputs)\n",
    "        intent_loss = loss_fn_intent(labels['intent_labels'], intent_logits)\n",
    "        entity_loss = loss_fn_entity(labels['entity_labels'], ner_logits)\n",
    "        \n",
    "        total_loss = intent_loss + entity_loss\n",
    "        \n",
    "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataset):\n",
    "        inputs, labels = batch\n",
    "        loss = train_step(inputs, labels)\n",
    "        total_loss += loss\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss.numpy():.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save_pretrained('../models/multitask_bert_tf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
